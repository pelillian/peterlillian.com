<!doctype html>
<html data-n-head-ssr>
  <head>
    <title>Thoughts | Peter Lillian</title><meta data-n-head="ssr" charset="utf-8"><meta data-n-head="ssr" name="viewport" content="width=device-width,initial-scale=1"><meta data-n-head="ssr" data-hid="description" name="description" content=""><meta data-n-head="ssr" name="msapplication-TileColor" content="#180030"><meta data-n-head="ssr" name="theme-color" content="#180030"><link data-n-head="ssr" rel="icon" type="image/x-icon" href="/favicon.ico"><link data-n-head="ssr" rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link data-n-head="ssr" rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"><link data-n-head="ssr" rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link data-n-head="ssr" rel="manifest" href="/site.webmanifest"><link data-n-head="ssr" rel="mask-icon" href="/safari-pinned-tab.svg" color="#180030"><link rel="preload" href="/_nuxt/1861c5a.js" as="script"><link rel="preload" href="/_nuxt/9d95d4c.js" as="script"><link rel="preload" href="/_nuxt/53b73e7.js" as="script"><link rel="preload" href="/_nuxt/6ceb292.js" as="script"><link rel="preload" href="/_nuxt/f21693c.js" as="script"><link rel="preload" href="/_nuxt/462953a.js" as="script"><style data-vue-ssr-id="d5573df6:0 517a8dd7:0 fa7ff0ca:0 30d029b9:0 b4c0b796:0">@font-face{font-family:"League Spartan";src:url(/_nuxt/fonts/LeagueSpartan-VF.b6db7c2.woff2) format("woff2-variations");font-weight:200 900}html{font-family:"League Spartan",-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,sans-serif;font-weight:400;font-size:18px;line-height:1.4;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;box-sizing:border-box;background:#180030;color:#fff}*,:after,:before{box-sizing:border-box;margin:0}.container{margin:0 auto;min-height:100vh;max-width:1000px;padding:15vh 0;width:60vw;display:flex;justify-content:center;align-items:center;text-align:center}.right{text-align:right;float:right}h1{font-size:100px;letter-spacing:10px}h1,h2{display:block;font-weight:700}h2{font-size:50px}a{font-weight:700;color:#e7bf8f;text-decoration:none}a:hover{text-decoration:underline}.button{font-size:50px;font-weight:700;position:relative;display:inline-block;margin:20px;color:#fff;text-decoration:none;cursor:pointer;padding:10px 30px;transform:scale(.8);transition:all .3s ease;transition-property:color transform}.button.small{font-size:36px}.button .button-text{position:relative}.button .button-text:after{padding:0!important}.button:focus,.button:hover{color:#180030;text-decoration:none;transform:scale(1)}.button:focus .button-text:after,.button:focus:after,.button:hover .button-text:after,.button:hover:after{opacity:1}.button .button-text:after,.button:after{content:attr(data-after);position:absolute;z-index:5;opacity:0;left:0;bottom:0;padding:10px 30px;height:100%;width:100%;background:linear-gradient(130deg,#734a1f,#d59748,#e7bf8f,#d59748);background-clip:text;-webkit-background-clip:text;-webkit-text-fill-color:transparent;-moz-background-clip:text;-moz-text-fill-color:transparent;transition:opacity .3s ease}.item{font-weight:400;text-align:left;margin-top:48px}.item p{margin-top:18px}.item p b{font-size:24px}.item pre{margin-top:9px;font-family:inherit}.item .indent{margin-left:40px}.item.center{text-align:center}.page-enter-active,.page-leave-active{transition:transform .5s ease,opacity .5s ease!important}.page-enter{opacity:0;transform:translate(30px,30px)}.page-leave-to{opacity:0;transform:translate(-30px,-30px)}code[class*=language-],pre[class*=language-]{color:#000;background:0 0;text-shadow:0 1px #fff;font-family:Consolas,Monaco,"Andale Mono","Ubuntu Mono",monospace;font-size:1em;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-ms-hyphens:none;hyphens:none}code[class*=language-] ::-moz-selection,code[class*=language-]::-moz-selection,pre[class*=language-] ::-moz-selection,pre[class*=language-]::-moz-selection{text-shadow:none;background:#b3d4fc}code[class*=language-] ::selection,code[class*=language-]::selection,pre[class*=language-] ::selection,pre[class*=language-]::selection{text-shadow:none;background:#b3d4fc}@media print{code[class*=language-],pre[class*=language-]{text-shadow:none}}pre[class*=language-]{padding:1em;margin:.5em 0;overflow:auto}:not(pre)>code[class*=language-],pre[class*=language-]{background:#f5f2f0}:not(pre)>code[class*=language-]{padding:.1em;border-radius:.3em;white-space:normal}.token.cdata,.token.comment,.token.doctype,.token.prolog{color:#708090}.token.punctuation{color:#999}.token.namespace{opacity:.7}.token.boolean,.token.constant,.token.deleted,.token.number,.token.property,.token.symbol,.token.tag{color:#905}.token.attr-name,.token.builtin,.token.char,.token.inserted,.token.selector,.token.string{color:#690}.language-css .token.string,.style .token.string,.token.entity,.token.operator,.token.url{color:#9a6e3a;background:hsla(0,0%,100%,.5)}.token.atrule,.token.attr-value,.token.keyword{color:#07a}.token.class-name,.token.function{color:#dd4a68}.token.important,.token.regex,.token.variable{color:#e90}.token.bold,.token.important{font-weight:700}.token.italic{font-style:italic}.token.entity{cursor:help}.nuxt-progress{position:fixed;top:0;left:0;right:0;height:2px;width:0;opacity:1;transition:width .1s,opacity .4s;background-color:#000;z-index:999999}.nuxt-progress.nuxt-progress-notransition{transition:none}.nuxt-progress-failed{background-color:red}[data-v-3c5fdbda] .nuxt-content p{max-width:36rem;margin:0 auto 30px;text-align:left}[data-v-3c5fdbda] .nuxt-content figure{margin:0 auto 30px}[data-v-3c5fdbda] .nuxt-content figure img{max-height:400px;max-width:36rem;border-radius:40px}[data-v-3c5fdbda] .nuxt-content figure figcaption{max-width:28rem;margin:auto}[data-v-3c5fdbda] .nuxt-content blockquote{font-style:italic;max-width:28rem;margin:30px auto}[data-v-3c5fdbda] .nuxt-content blockquote p{margin:0 auto 1rem}@media screen and (min-width:800px){.links[data-v-fa47d306]{display:flex;flex-direction:row}}.links[data-v-fa47d306]{margin:30px auto;max-width:800px;width:100%;justify-content:center;align-items:center;align-content:center}.links .button[data-v-fa47d306]{margin:5px 0 0}</style><link rel="preload" href="/_nuxt/static/1610157040/thoughts/automated/payload.js" as="script"><link rel="preload" href="/_nuxt/static/1610157040/manifest.js" as="script">
  </head>
  <body>
    <div data-server-rendered="true" id="__nuxt"><!----><div id="__layout"><div style="transform:translate3d(0,0,-1000px) perspective(800px) rotateX(0) rotateY(0);transform-origin:50% 0"><div class="container" data-v-3c5fdbda><div data-v-3c5fdbda><div class="links" data-v-fa47d306 data-v-3c5fdbda><a href="/thoughts" data-after="◀   BACK" class="button nuxt-link-active" data-v-fa47d306>◀   BACK</a></div> <h1 data-v-3c5fdbda>AUTOMATED AUTHORITY</h1> <div class="nuxt-content" data-v-3c5fdbda data-v-3c5fdbda><h2 id="peter-lillian" data-v-3c5fdbda data-v-3c5fdbda><a href="#peter-lillian" aria-hidden="true" tabindex="-1" data-v-3c5fdbda data-v-3c5fdbda><span class="icon icon-link" data-v-3c5fdbda data-v-3c5fdbda></span></a>PETER LILLIAN</h2>
<h2 id="july-2019" data-v-3c5fdbda data-v-3c5fdbda><a href="#july-2019" aria-hidden="true" tabindex="-1" data-v-3c5fdbda data-v-3c5fdbda><span class="icon icon-link" data-v-3c5fdbda data-v-3c5fdbda></span></a>JULY 2019</h2>
<blockquote data-v-3c5fdbda data-v-3c5fdbda>
<p data-v-3c5fdbda data-v-3c5fdbda>When Marduk commanded me to give justice to the people of
the land and to let them have good governance, I set forth...
to make justice appear in the land, to destroy the evil and
the wicked, that the strong might not oppress the weak.</p>
<p data-v-3c5fdbda data-v-3c5fdbda>---Hammurabi of Babylonia [1]</p>
</blockquote>
<p data-v-3c5fdbda data-v-3c5fdbda>All governments throughout history have been run by humans. Some have
claimed to serve the people of their society—others have blatantly
served a different goal. In the modern era, and governments play a
larger role in human life than ever before—everything from education to
urban planning is a function of the state. But have the motivations of
governments improved? The Bulletin of Atomic Scientists considers the
current political climate "as worrisome as the most dangerous times of
the Cold War" [2].</p>
<figure data-v-3c5fdbda data-v-3c5fdbda>
<img src="/_nuxt/img/babylon.66c32e4.jpg" alt=" Marduk, ruler of the gods, gives legal power to Hammurabi, king of Babylonia [3]" data-v-3c5fdbda><figcaption aria-hidden="true" data-v-3c5fdbda data-v-3c5fdbda> <em data-v-3c5fdbda data-v-3c5fdbda>Marduk, ruler of the gods, gives legal power to Hammurabi, king of Babylonia</em> <span class="citation" data-cites="babylon" data-v-3c5fdbda data-v-3c5fdbda>[3]</span></figcaption>
</figure>
<p data-v-3c5fdbda data-v-3c5fdbda>Climate change has gotten to the point where many experts consider it
unavoidable [4]. Wealth inequality throughout the developed world has
been getting worse [5]. Humans tend to be self-serving, fail to think
long-term, and exhibit bias towards members of their own groups, among
many other pitfalls [6]. It is this kind of flawed decision-making,
typical of <em data-v-3c5fdbda data-v-3c5fdbda>Homo sapiens</em>, that causes so much calamity.</p>
<p data-v-3c5fdbda data-v-3c5fdbda>With these flaws, we won’t be ruling the Earth forever. Artificial
Intelligence is improving by the day. According to the International
Data Corporation, global spending on AI is expected to double by 2022 to
almost $80 billion [7]. In the wake of this windfall, we’ll start to
see more intelligent systems—machines eventually capable of performing
all tasks better than humans [8]. What then? The stakeholders in the
future of AI include not only the human race, but also life on Earth and
potentially the entire universe—we can use the utility ethics test to
examine these scenarios. Should we put limits on AI to keep humans
employed? A Luddite approach will only hold us back, as governments or
corporations that implement these kind of regulations will inevitably be
outcompeted by those that do not. Even if the whole world got together
and agreed to put anti-AI laws into place (good luck getting North Korea
or the Mafia to agree to that anyway), what would be the purpose of
making humans do jobs robots can do better? Do we still have people draw
spreadsheets by hand?</p>
<figure data-v-3c5fdbda data-v-3c5fdbda>
<img src="/_nuxt/img/luddites.d1caa4b.jpg" alt=" Luddites, a group of textile workers, destroy the textile mills that replaced their jobs. [9]" data-v-3c5fdbda><figcaption aria-hidden="true" data-v-3c5fdbda data-v-3c5fdbda> <em data-v-3c5fdbda data-v-3c5fdbda>Luddites, a group of textile workers, destroy the textile mills that replaced their jobs.</em> <span class="citation" data-cites="luddites" data-v-3c5fdbda data-v-3c5fdbda>[9]</span></figcaption>
</figure>
<p data-v-3c5fdbda data-v-3c5fdbda>Of course, Microsoft Excel isn’t going to revolt against us anytime
soon, but for the purposes of this paper we will assume that we have
solved the superintelligent control problem (as this is a technical
challenge) and are able to create friendly AI that won’t harm us.<sup data-v-3c5fdbda data-v-3c5fdbda>1</sup>
There is one job, though, that might seem smart to keep as a human
occupation. Leadership—an especially valued human quality—will likely be
the last job handed over to the machines. For our own sakes, it
shouldn’t be. Though it may scare some people, putting AI into power
will make a smarter, faster, and more equitable world.</p>
<p data-v-3c5fdbda data-v-3c5fdbda>In order to examine these questions, we can look at an automated
authority from the perspective of the utility test. With a machine
government, the average person would benefit financially from an
improved economy—the AI’s superior financial policy would make
recessions much less likely and put more money in the pockets of
everyday people [11]. In this society, as no humans work, the machines
will remove all social classes. With no humans working, it will be
impossible to maintain that some humans deserve more wealth. However,
this would anger the rich, as their institutions and old-boy networks
within the financial system would no longer give them power. In the
long-term, though, the incredible abundance produced by a post-scarcity
economy (one where robots create endless wealth distributed equally
among all humans) might allow everyone the type of luxuriant lifestyle
previously only afforded to the few [12].</p>
<p data-v-3c5fdbda data-v-3c5fdbda>We could also release the source code of this AI so that any interested
citizen could literally read the mind of their leader, and even vote on
tweaks or improvements. This AI could be programmed to always act in the
perfect interest of the people [10]. If this AI’s programming was made
open-source, the average person would have much more confidence in their
government, something that is sorely lacking in many human regimes
today. Of course, while an AI government is great for the citizens, it
would be a nightmare for politicians as they would be out of a
job—though this might not be all bad, as the European Center for the
Governance of Change found that already 1/4 of citizens would prefer AI
to their current politicians [13].</p>
<p data-v-3c5fdbda data-v-3c5fdbda>A true superintelligence could respond to threats instantly [10]. An
economic crash or surprise war would take human leaders off-guard, but
an AI would be able to watch the situation unfold at millions of frames
per second. The downside to this, though, is that the public won’t be
able to react to its decisions in time to modify it. We could create
robust guaranteed-friendly AI (that won’t kill us), but many members of
the public would still hold reservations about this situation, and this
could make them more unhappy under the new government. Yet after
generations of successful machine rule, it is unlikely that people would
continue to harbor these views.</p>
<p data-v-3c5fdbda data-v-3c5fdbda>One task humans are notoriously bad at is long-term planning [14], a
kind of thinking that is a natural strength of machines [15].
Avoidable catastrophes like climate change, which our governments have
utterly failed to address [4], would not happen to civilizations run
by AI, as they would be built with real long-term planning abilities.
This means that the Earth and the environment would greatly benefit from
governments actually understanding how risky actions like logging the
Amazon can be. With this kind of planning, governments will be able to
address existential risks<sup data-v-3c5fdbda data-v-3c5fdbda>2</sup> like asteroid impacts or global pandemics
[16]. An AI government would also be adept at finding and dealing with
threats we can’t forsee, such as dangers from other disruptive
technologies (e.g. nanotech) or gamma ray bursts, any of which could
easily wipe out life on earth [16].</p>
<figure data-v-3c5fdbda data-v-3c5fdbda>
<img src="/_nuxt/img/fire.5d31be3.jpg" alt=" Firefighters tackle a blaze near Boston, MA. Humans tend to solve the short-term problem (fight the fire) but not the long-term problem (end climate change). [17]" data-v-3c5fdbda><figcaption aria-hidden="true" data-v-3c5fdbda data-v-3c5fdbda> <em data-v-3c5fdbda data-v-3c5fdbda>Firefighters tackle a blaze near Boston, MA. Humans tend to solve the short-term problem (fight the fire) but not the long-term problem (end climate change).</em> <span class="citation" data-cites="fire" data-v-3c5fdbda data-v-3c5fdbda>[17]</span></figcaption>
</figure>
<p data-v-3c5fdbda data-v-3c5fdbda>If we decided not to implement an AI government, a very different set of
outcomes emerges. The biggest immediate issue? AI use spreads to almost
all industries; businesses realize to stay competitive they need to
replace their CEOs with machines [10]. This means that suddenly we
have a government, which is tasked with the regulation and control of
private industry, trying to keep up with AI that evolve by the hour.
Think about how tech-illiterate our current politicians are, unable to
ask relevant questions at Facebook hearings [18]. Imagine them trying
to control organizations run by AI at the speed of thought. They’d be
outclassed like a monkey trying to run the circus.</p>
<p data-v-3c5fdbda data-v-3c5fdbda>With government oversight of the economy made impossible, the
ultra-wealthy will be able to further entrench their position. As robots
start to outperform on all tasks, their owners—large corporations—will
reap the profits instead of the average person. In the worst case the
entirety of wealth could be controlled by a few robotic
mega-corporations [10]. If the government is not also using the power
of AI, these superintelligent behemoths will be able to manipulate our
leaders at their whims. That would take us down a dark path.
Historically, the power of the citizen has been determined by their
share of the economic output [19]. Countries with educated and
economically productive citizenship become democracies because the
government needs its citizens for tax revenue, while countries with rich
natural resources (oil) become autocracies because the government
doesn’t need its citizens [20]. As machines replace all human labor,
there will be no longer an incentive for those in power to share wealth.</p>
<figure data-v-3c5fdbda data-v-3c5fdbda>
<img src="/_nuxt/img/rome.06d22a0.jpg" alt=" The wealthy Roman aristocrats, having appropriated much of the wealth of the Empire, live in decadent, luxurious paradise. [21]" data-v-3c5fdbda><figcaption aria-hidden="true" data-v-3c5fdbda data-v-3c5fdbda> <em data-v-3c5fdbda data-v-3c5fdbda>The wealthy Roman aristocrats, having appropriated much of the wealth of the Empire, live in decadent, luxurious paradise.</em> <span class="citation" data-cites="rome" data-v-3c5fdbda data-v-3c5fdbda>[21]</span></figcaption>
</figure>
<p data-v-3c5fdbda data-v-3c5fdbda>Without an AI government bound by programming to serve the people, the
chance whatever is left of the government falling into a nightmare
autocracy worsens. Needless to say, this is not a situation where the
average person benefits.</p>
<p data-v-3c5fdbda data-v-3c5fdbda>In this future, whatever superpower has faster AI will have an extreme
advantage in wartime, and may be able to dominate the rest of the world.
Researchers at UC Berkeley believe that it will "lower the threshold for
going to war by making it possible to attack an enemy while incurring no
immediate risk" [22]. This means that any country without an AI
government able to make rapid and effective decisions would be putting
itself in serious jeopardy of invasion. This could have obviously
extreme negative consequences for all stakeholders in countries without
AI leaders, in both the short- and long-term (if there is a long-term).</p>
<p data-v-3c5fdbda data-v-3c5fdbda>But this entire paper might be useless: AI may be better at the task of
ethics itself. AI researcher Nick Bostrom writes, "To the extent that
ethics is a cognitive pursuit, a superintelligence could do it better
than human thinkers." [15] This means AI should be the one doing the
ethical analysis of whether or not to put an AI in power. Again,
assuming we solve the control problem, this paper will be made obsolete
by future machines’ work. Perhaps I could have saved the effort and
simply written "Let future AI solve this."</p>
<p data-v-3c5fdbda data-v-3c5fdbda>Human ethics have developed over a long history of dealing with specific
moral situations involving other humans [23]. The future, however,
will certainly involve intelligent beings that are nonhuman. When AI
becomes smarter than us, we will have to think about its place in
society—will we give decision-making powers to a machine? It seems that
the answer is uncomfortably clear: there is really no choice other than
some form of AI government. In the end, it may actually not matter what
we choose, as either our current government will evolve into an AI as we
automate, or a superintelligent AI programmed to help humanity will take
power for our own good. In any case, machines are coming to rule us: if
we do it right, it might not be as bad as you think.</p>
<p data-v-3c5fdbda data-v-3c5fdbda><span class="csl-left-margin" data-v-3c5fdbda data-v-3c5fdbda>[1] </span><span class="csl-right-inline" data-v-3c5fdbda data-v-3c5fdbda>G. R. Driver and J. C. Miles, <em data-v-3c5fdbda data-v-3c5fdbda>The babylonian
laws</em>. Wipf; Stock Publishers, 2007.</span></p>
<p data-v-3c5fdbda data-v-3c5fdbda><span class="csl-left-margin" data-v-3c5fdbda data-v-3c5fdbda>[2] </span><span class="csl-right-inline" data-v-3c5fdbda data-v-3c5fdbda>J. Mecklin, “A new abnormal: It is still 2
minutes to midnight, 2019 doomsday clock statement.” Chicago, IL:
Science; Security Board, Bulletin of the Atomic Scientists, 2019.</span></p>
<p data-v-3c5fdbda data-v-3c5fdbda><span class="csl-left-margin" data-v-3c5fdbda data-v-3c5fdbda>[3] </span><span class="csl-right-inline" data-v-3c5fdbda data-v-3c5fdbda>DK Images, “Stone tablet, hammurabi’s laws.”
2019, [Online]. Available:
<a href="https://res.cloudinary.com/dk-find-out/image/upload/q_80,w_1440,f_auto/A-Almy-BPDGW6_q4heaf.jpg" rel="nofollow noopener noreferrer" target="_blank" data-v-3c5fdbda data-v-3c5fdbda>https://res.cloudinary.com/dk-find-out/image/upload/q_80,w_1440,f_auto/A-Almy-BPDGW6_q4heaf.jpg</a>.</span></p>
<p data-v-3c5fdbda data-v-3c5fdbda><span class="csl-left-margin" data-v-3c5fdbda data-v-3c5fdbda>[4] </span><span class="csl-right-inline" data-v-3c5fdbda data-v-3c5fdbda>A. Ghosh, <em data-v-3c5fdbda data-v-3c5fdbda>The great derangement: Climate
change and the unthinkable</em>. Penguin UK, 2018.</span></p>
<p data-v-3c5fdbda data-v-3c5fdbda><span class="csl-left-margin" data-v-3c5fdbda data-v-3c5fdbda>[5] </span><span class="csl-right-inline" data-v-3c5fdbda data-v-3c5fdbda>K. W. Knight, J. B. Schor, and A. K. Jorgenson,
“Wealth inequality and carbon emissions in high-income countries,”
<em data-v-3c5fdbda data-v-3c5fdbda>Social Currents</em>, vol. 4, no. 5, pp. 403–412, 2017.</span></p>
<p data-v-3c5fdbda data-v-3c5fdbda><span class="csl-left-margin" data-v-3c5fdbda data-v-3c5fdbda>[6] </span><span class="csl-right-inline" data-v-3c5fdbda data-v-3c5fdbda>M. G. Haselton and D. Nettle, “The paranoid
optimist: An integrative evolutionary model of cognitive biases,”
<em data-v-3c5fdbda data-v-3c5fdbda>Personality and social psychology Review</em>, vol. 10, no. 1, pp. 47–66,
2006.</span></p>
<p data-v-3c5fdbda data-v-3c5fdbda><span class="csl-left-margin" data-v-3c5fdbda data-v-3c5fdbda>[7] </span><span class="csl-right-inline" data-v-3c5fdbda data-v-3c5fdbda>International Data Corporation, “Worldwide
semiannual artificial intelligence systems spending guide,” <em data-v-3c5fdbda data-v-3c5fdbda>IDC Media</em>,
2019.</span></p>
<p data-v-3c5fdbda data-v-3c5fdbda><span class="csl-left-margin" data-v-3c5fdbda data-v-3c5fdbda>[8] </span><span class="csl-right-inline" data-v-3c5fdbda data-v-3c5fdbda>N. Bostrom, “A history of transhumanist
thought,” <em data-v-3c5fdbda data-v-3c5fdbda>Journal of evolution and technology</em>, vol. 14, no. 1,
2005.</span></p>
<p data-v-3c5fdbda data-v-3c5fdbda><span class="csl-left-margin" data-v-3c5fdbda data-v-3c5fdbda>[9] </span><span class="csl-right-inline" data-v-3c5fdbda data-v-3c5fdbda>C. I. Inc., “Almanac: The luddites,” <em data-v-3c5fdbda data-v-3c5fdbda>CBS</em>,
Mar. 2018.</span></p>
<p data-v-3c5fdbda data-v-3c5fdbda><span class="csl-left-margin" data-v-3c5fdbda data-v-3c5fdbda>[10] </span><span class="csl-right-inline" data-v-3c5fdbda data-v-3c5fdbda>N. Bostrom, <em data-v-3c5fdbda data-v-3c5fdbda>Superintelligence: Paths, dangers,
strategies</em>. Oxford University Press, 2014.</span></p>
<p data-v-3c5fdbda data-v-3c5fdbda><span class="csl-left-margin" data-v-3c5fdbda data-v-3c5fdbda>[11] </span><span class="csl-right-inline" data-v-3c5fdbda data-v-3c5fdbda>E. Ernst, R. Merola, and D. Samaan, “The
economics of artificial intelligence: Implications for the future of
work,” <em data-v-3c5fdbda data-v-3c5fdbda>ILO Future of Work Research Paper Series</em>, vol. 5, p. 41,
2018.</span></p>
<p data-v-3c5fdbda data-v-3c5fdbda><span class="csl-left-margin" data-v-3c5fdbda data-v-3c5fdbda>[12] </span><span class="csl-right-inline" data-v-3c5fdbda data-v-3c5fdbda>A. Korinek and J. E. Stiglitz, “Artificial
intelligence and its implications for income distribution and
unemployment,” National Bureau of Economic Research, 2017.</span></p>
<p data-v-3c5fdbda data-v-3c5fdbda><span class="csl-left-margin" data-v-3c5fdbda data-v-3c5fdbda>[13] </span><span class="csl-right-inline" data-v-3c5fdbda data-v-3c5fdbda>IE University, “European tech insights 2019,”
<em data-v-3c5fdbda data-v-3c5fdbda>Center for the Governance of Change</em>, Mar. 2019, [Online]. Available:
<a href="https://www.ie.edu/cgc/research/tech-opinion-poll-2019/" rel="nofollow noopener noreferrer" target="_blank" data-v-3c5fdbda data-v-3c5fdbda>https://www.ie.edu/cgc/research/tech-opinion-poll-2019/</a>.</span></p>
<p data-v-3c5fdbda data-v-3c5fdbda><span class="csl-left-margin" data-v-3c5fdbda data-v-3c5fdbda>[14] </span><span class="csl-right-inline" data-v-3c5fdbda data-v-3c5fdbda>D. Dörner and H. Schaub, “Errors in planning
and decision-making and the nature of human information processing,”
<em data-v-3c5fdbda data-v-3c5fdbda>Applied psychology</em>, vol. 43, no. 4, pp. 433–453, 1994.</span></p>
<p data-v-3c5fdbda data-v-3c5fdbda><span class="csl-left-margin" data-v-3c5fdbda data-v-3c5fdbda>[15] </span><span class="csl-right-inline" data-v-3c5fdbda data-v-3c5fdbda>N. Bostrom, “Ethical issues in advanced
artificial intelligence,” <em data-v-3c5fdbda data-v-3c5fdbda>Science Fiction and Philosophy: From Time
Travel to Superintelligence</em>, pp. 277–284, 2003.</span></p>
<p data-v-3c5fdbda data-v-3c5fdbda><span class="csl-left-margin" data-v-3c5fdbda data-v-3c5fdbda>[16] </span><span class="csl-right-inline" data-v-3c5fdbda data-v-3c5fdbda>N. Bostrom and M. M. Cirkovic, <em data-v-3c5fdbda data-v-3c5fdbda>Global
catastrophic risks</em>. Oxford University Press, 2011.</span></p>
<p data-v-3c5fdbda data-v-3c5fdbda><span class="csl-left-margin" data-v-3c5fdbda data-v-3c5fdbda>[17] </span><span class="csl-right-inline" data-v-3c5fdbda data-v-3c5fdbda>T. Economist, “The world is losing the war
against climate change,” <em data-v-3c5fdbda data-v-3c5fdbda>The Economist</em>, Aug. 2018.</span></p>
<p data-v-3c5fdbda data-v-3c5fdbda><span class="csl-left-margin" data-v-3c5fdbda data-v-3c5fdbda>[18] </span><span class="csl-right-inline" data-v-3c5fdbda data-v-3c5fdbda>C. Rampell, “Our politicians have no idea how
the internet works,” <em data-v-3c5fdbda data-v-3c5fdbda>The Washington Post</em>, Aug. 2018.</span></p>
<p data-v-3c5fdbda data-v-3c5fdbda><span class="csl-left-margin" data-v-3c5fdbda data-v-3c5fdbda>[19] </span><span class="csl-right-inline" data-v-3c5fdbda data-v-3c5fdbda>D. Acemoglu and J. A. Robinson, “Persistence of
power, elites, and institutions,” <em data-v-3c5fdbda data-v-3c5fdbda>American Economic Review</em>, vol. 98,
no. 1, pp. 267–93, 2008.</span></p>
<p data-v-3c5fdbda data-v-3c5fdbda><span class="csl-left-margin" data-v-3c5fdbda data-v-3c5fdbda>[20] </span><span class="csl-right-inline" data-v-3c5fdbda data-v-3c5fdbda>M. L. Ross, “Does oil hinder democracy?” <em data-v-3c5fdbda data-v-3c5fdbda>World
politics</em>, vol. 53, no. 3, pp. 325–361, 2001.</span></p>
<p data-v-3c5fdbda data-v-3c5fdbda><span class="csl-left-margin" data-v-3c5fdbda data-v-3c5fdbda>[21] </span><span class="csl-right-inline" data-v-3c5fdbda data-v-3c5fdbda>T. Couture, “Romans during the decadence.”
1847, [Online]. Available:
<a href="https://commons.wikimedia.org/wiki/File:Thomas_Couture_-_Romans_during_the_Decadence_-_Google_Art_Project.jpg" rel="nofollow noopener noreferrer" target="_blank" data-v-3c5fdbda data-v-3c5fdbda>https://commons.wikimedia.org/wiki/File:Thomas_Couture_-_Romans_during_the_Decadence_-_Google_Art_Project.jpg</a>.</span></p>
<p data-v-3c5fdbda data-v-3c5fdbda><span class="csl-left-margin" data-v-3c5fdbda data-v-3c5fdbda>[22] </span><span class="csl-right-inline" data-v-3c5fdbda data-v-3c5fdbda>S. Russell, S. Hauert, R. Altman, and M.
Veloso, “Ethics of artificial intelligence,” <em data-v-3c5fdbda data-v-3c5fdbda>Nature</em>, vol. 521, no.
7553, pp. 415–416, 2015.</span></p>
<p data-v-3c5fdbda data-v-3c5fdbda><span class="csl-left-margin" data-v-3c5fdbda data-v-3c5fdbda>[23] </span><span class="csl-right-inline" data-v-3c5fdbda data-v-3c5fdbda>A. MacIntyre, <em data-v-3c5fdbda data-v-3c5fdbda>A short history of ethics: A
history of moral philosophy from the homeric age to the 20th century</em>.
Routledge, 2003.</span></p>
<p data-v-3c5fdbda data-v-3c5fdbda><sup data-v-3c5fdbda data-v-3c5fdbda>1</sup> A discussion of the dangers of unfriendly AI is out of scope here,
but the curious reader will find Nick Bostrom’s <em data-v-3c5fdbda data-v-3c5fdbda>Superintelligence:
Paths, Dangers, Strategies</em> [10] to be an enlightening overview of the
topic.</p>
<p data-v-3c5fdbda data-v-3c5fdbda><sup data-v-3c5fdbda data-v-3c5fdbda>2</sup> Existential risks are risks to the continued survival of life on
this planet or in the universe [16].</p></div> <div class="links" data-v-fa47d306 data-v-3c5fdbda><a href="/thoughts" data-after="◀   BACK" class="button nuxt-link-active" data-v-fa47d306>◀   BACK</a></div></div></div></div></div></div><script>window.__NUXT__={staticAssetsBase:"/_nuxt/static/1610157040",layout:"default",error:null,serverRendered:!0,routePath:"/thoughts/automated",config:{content:{dbHash:"71e62ee1"}}}</script><script src="/_nuxt/1861c5a.js" defer></script><script src="/_nuxt/f21693c.js" defer></script><script src="/_nuxt/462953a.js" defer></script><script src="/_nuxt/9d95d4c.js" defer></script><script src="/_nuxt/53b73e7.js" defer></script><script src="/_nuxt/6ceb292.js" defer></script>
  </body>
</html>
